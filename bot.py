import os
import requests
import urllib.parse
from bs4 import BeautifulSoup
from newspaper import Article
from google import genai
from dotenv import load_dotenv
import trafilatura
import time
import datetime
import yfinance as yf
from supabase import create_client, Client
import random

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
TOKEN = os.environ.get("TELEGRAM_TOKEN")
CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")
GEMINI_API_KEY = os.environ.get("GOOGLE_API_KEY")
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

# 2. Supabase ì—°ê²°
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY) if SUPABASE_URL and SUPABASE_KEY else None

# --- [ìœ í‹¸ë¦¬í‹°] í…”ë ˆê·¸ë¨ ì „ì†¡ ---
def send_telegram(text):
    if not TOKEN or not CHAT_ID: return
    url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
    data = {"chat_id": CHAT_ID, "text": text, "parse_mode": "HTML", "disable_web_page_preview": "true"}
    try: requests.post(url, data=data, timeout=5)
    except Exception as e: print(f"ì „ì†¡ ì‹¤íŒ¨: {e}")

# --- [ìœ í‹¸ë¦¬í‹°] DB ì¡°íšŒ ---
def get_db_data():
    if not supabase: return [], {}
    try:
        response = supabase.table('keywords').select("*").eq('is_active', True).execute()
        rows = response.data
        active_keywords = []
        ticker_map = {}
        for item in rows:
            word = item.get('keyword')
            ticker = item.get('ticker')
            if word:
                active_keywords.append(word)
                if ticker: ticker_map[word] = ticker
        return active_keywords, ticker_map
    except Exception as e:
        print(f"âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return [], {}

# --- [ìœ í‹¸ë¦¬í‹°] ì£¼ê°€ ì •ë³´ ì¡°íšŒ ---
def get_stock_info(keyword, ticker_map):
    ticker = ticker_map.get(keyword)
    if not ticker: return ""
    try:
        stock = yf.Ticker(ticker)
        info = stock.fast_info
        price = info.last_price
        if price is None: return ""
        
        change = price - info.previous_close
        change_pct = (change / info.previous_close) * 100
        emoji = "ğŸ”º" if change > 0 else "ğŸ¦‹" if change < 0 else "â–"
        
        vol_str = ""
        if info.last_volume and info.three_month_average_volume:
            vol_ratio = (info.last_volume / info.three_month_average_volume) * 100
            vol_stat = "ğŸ”¥í­ë°œ" if vol_ratio >= 200 else "â–í‰ì´"
            vol_str = f"ê±°ë˜ëŸ‰: {vol_stat} ({vol_ratio:.0f}%)\n"
        
        result = f"\nğŸ’° <b>{keyword} ì‹œì¥ í˜„í™©</b>\n{'-'*20}\n"
        result += f"í˜„ì¬ê°€: {price:,.0f} ({emoji} {change_pct:.2f}%)\n"
        result += vol_str + "\n"
        return result
    except: return ""

# --- [í•µì‹¬] ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„ (êµ¬ê¸€ + ë¹™) ---
def fetch_rss_items(keyword):
    """êµ¬ê¸€ ë‰´ìŠ¤ë¥¼ ë¨¼ì € í„¸ê³ , ì—†ìœ¼ë©´ ë¹™ ë‰´ìŠ¤ë¥¼ í…ë‹ˆë‹¤."""
    encoded = urllib.parse.quote(keyword)
    items = []
    
    # 1. ê²€ìƒ‰ ì—”ì§„ ë¦¬ìŠ¤íŠ¸ (ìš°ì„ ìˆœìœ„: êµ¬ê¸€ -> ë¹™)
    search_urls = [
        ("Google", f"https://news.google.com/rss/search?q={encoded}&hl=ko&gl=KR&ceid=KR:ko"),
        ("Bing", f"https://www.bing.com/news/search?q={encoded}&format=rss")
    ]
    
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

    for source_name, url in search_urls:
        if len(items) >= 4: break # ì´ë¯¸ ì¶©ë¶„í•˜ë©´ ì¤‘ë‹¨
        try:
            print(f"ğŸ“¡ {source_name} ê²€ìƒ‰ ì‹œë„...")
            res = requests.get(url, headers=headers, timeout=5)
            soup = BeautifulSoup(res.text, "xml")
            found_items = soup.find_all("item")
            
            for item in found_items:
                title = item.title.get_text()
                link = item.link.get_text()
                # RSSì— ìˆëŠ” ìš”ì•½ë³¸(description) ì¶”ì¶œ
                snippet = ""
                if item.description:
                    snippet = BeautifulSoup(item.description.get_text(), "html.parser").get_text()
                
                items.append({"source": source_name, "title": title, "link": link, "snippet": snippet})
                if len(items) >= 4: break
        except Exception as e:
            print(f"âš ï¸ {source_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
    return items

# --- [í•µì‹¬] ë³¸ë¬¸ ì¶”ì¶œ ì—”ì§„ (Trafilatura + Newspaper3k) ---
def get_article_content(url):
    """ì—¬ëŸ¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë™ì›í•´ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„"""
    # 1. Trafilatura ì‹œë„ (ê°€ì¥ ê¹”ë”í•¨)
    try:
        d = trafilatura.fetch_url(url)
        if d:
            t = trafilatura.extract(d, include_comments=False, include_tables=False)
            if t and len(t) > 50: return t[:1500]
    except: pass
    
    # 2. Newspaper3k ì‹œë„ (ì „í†µì˜ ê°•ì)
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        a = Article(url, language='ko', browser_user_agent=headers['User-Agent'])
        a.download()
        a.parse()
        if len(a.text) > 50: return a.text[:1500]
    except: pass

    return None # ë‹¤ ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜

# --- [í•µì‹¬] AI ìš”ì•½ (í•œê¸€ ê°•ì œ) ---
def get_gemini_summary(keyword, text_data):
    if not GEMINI_API_KEY: return "âš ï¸ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        client = genai.Client(api_key=GEMINI_API_KEY)
        prompt = f"""
        ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ í€ë“œë§¤ë‹ˆì €ì´ì ì‹œì¥ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
        ì œê³µëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{keyword}' ì¢…ëª©ì— ëŒ€í•œ íˆ¬ì ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.

        [ì§€ì‹œ ì‚¬í•­]
        1. ì–¸ì–´: **ë¬´ì¡°ê±´ í•œêµ­ì–´(Korean)**ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
        2. ì–´ì¡°: ì „ë¬¸ì ì´ê³  ê°ê´€ì ì´ë˜, ì •ì¤‘í•œ 'í•´ìš”ì²´'ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
        3. ì„œì‹: ì¤‘ìš” ìˆ«ìë‚˜ í‚¤ì›Œë“œëŠ” <b>íƒœê·¸ë¡œ êµµê²Œ í‘œì‹œí•˜ì‹­ì‹œì˜¤.

        [ì¶œë ¥ ì–‘ì‹]
        Part 1: âš¡ **3ì¤„ í•µì‹¬ ìš”ì•½** (ì´ëª¨ì§€ í™œìš©, í•µì‹¬ ì´ìŠˆ ìœ„ì£¼)
        Part 2: ğŸ“ **ìƒì„¸ ì‹œì¥ íë¦„** (300ì ë‚´ì™¸, ë“±ë½ì˜ ì›ì¸ê³¼ ë°°ê²½ ì„¤ëª…)

        [ë‰´ìŠ¤ ë°ì´í„°]
        {text_data}
        """
        return client.models.generate_content(model="gemini-2.0-flash", contents=prompt).text
    except Exception as e: return f"AI Error: {e}"

# --- ë©”ì¸ ë¡œì§ ---
def process_keyword(keyword, ticker_map):
    print(f"ğŸš€ Analyzing: {keyword}")
    today = datetime.datetime.now().strftime("%y/%m/%d")
    stock_msg = get_stock_info(keyword, ticker_map)
    
    # 1. ë‰´ìŠ¤ ìˆ˜ì§‘ (êµ¬ê¸€ -> ë¹™)
    news_items = fetch_rss_items(keyword)
    
    if not news_items: 
        return f"ğŸ’¤ {keyword}: ë‰´ìŠ¤ ì—†ìŒ (Google & Bing ëª¨ë‘ ì‹¤íŒ¨)"

    llm_input = []
    news_links = []
    
    # 2. ë³¸ë¬¸ ì¶”ì¶œ ë° ë°ì´í„° ì¡°ë¦½
    for i, item in enumerate(news_items):
        title = item['title']
        link = item['link']
        snippet = item['snippet']
        source = item['source']
        
        news_links.append(f"{i+1}. [{source}] <a href='{link}'>{title}</a>")
        
        # ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
        content = get_article_content(link)
        
        if content:
            # ë³¸ë¬¸ ì„±ê³µ ì‹œ
            llm_input.append(f"[ê¸°ì‚¬ {i+1}] ì œëª©: {title}\në‚´ìš©: {content}\n")
        else:
            # ë³¸ë¬¸ ì‹¤íŒ¨ ì‹œ -> RSS Snippet(ìš”ì•½) ì‚¬ìš©
            llm_input.append(f"[ê¸°ì‚¬ {i+1}] ì œëª©: {title}\nìš”ì•½(ì ‘ì†ë¶ˆê°€): {snippet}\n")

    # 3. AI ë¶„ì„
    # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê²½ê³ í•˜ì§€ë§Œ, snippetì´ë¼ë„ ìˆìœ¼ë©´ ì§„í–‰
    full_text = "\n".join(llm_input)
    if len(full_text) < 30:
        return f"âš ï¸ {keyword}: ë¶„ì„í•  ë°ì´í„° ë¶€ì¡±"

    summary = get_gemini_summary(keyword, full_text)
    
    msg = f"ğŸ”¥ <b>[{today}] {keyword} ë¸Œë¦¬í•‘</b> ğŸ”¥\n{stock_msg}{summary}\n\n<b>ğŸ“° ì£¼ìš” ë‰´ìŠ¤</b>\n" + "\n".join(news_links)
    send_telegram(msg)
    return f"âœ… {keyword} ë¸Œë¦¬í•‘ ì™„ë£Œ"

# --- ì•± ì—°ë™ìš© ---
def run_batch_briefing():
    targets, ticker_map = get_db_data()
    logs = []
    if not targets: return ["âš ï¸ í™œì„±í™”ëœ íƒ€ê²Ÿì´ ì—†ìŠµë‹ˆë‹¤."]
    
    for word in targets:
        try:
            log = process_keyword(word, ticker_map)
            logs.append(log)
        except Exception as e:
            logs.append(f"âŒ {word} ì—ëŸ¬: {e}")
        time.sleep(2)
    return logs

if __name__ == "__main__":
    run_batch_briefing()