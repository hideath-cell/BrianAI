import os
import requests
import urllib.parse
from bs4 import BeautifulSoup
from newspaper import Article
from google import genai
from dotenv import load_dotenv
import trafilatura
import time
import datetime
import yfinance as yf
from supabase import create_client, Client
import random

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
TOKEN = os.environ.get("TELEGRAM_TOKEN")
CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")
GEMINI_API_KEY = os.environ.get("GOOGLE_API_KEY")
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

# 2. Supabase ì—°ê²°
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY) if SUPABASE_URL and SUPABASE_KEY else None

# --- [ìœ í‹¸ë¦¬í‹°] í…”ë ˆê·¸ë¨ ì „ì†¡ ---
def send_telegram(text):
    if not TOKEN or not CHAT_ID: return
    url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
    data = {"chat_id": CHAT_ID, "text": text, "parse_mode": "HTML", "disable_web_page_preview": "true"}
    try: requests.post(url, data=data, timeout=5)
    except Exception as e: print(f"ì „ì†¡ ì‹¤íŒ¨: {e}")

# --- [ìœ í‹¸ë¦¬í‹°] DB ì¡°íšŒ ---
def get_db_data():
    if not supabase: return [], {}
    try:
        response = supabase.table('keywords').select("*").eq('is_active', True).execute()
        rows = response.data
        active_keywords = []
        ticker_map = {}
        for item in rows:
            word = item.get('keyword')
            ticker = item.get('ticker')
            if word:
                active_keywords.append(word)
                if ticker: ticker_map[word] = ticker
        return active_keywords, ticker_map
    except Exception as e:
        print(f"âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return [], {}

from quant_analyzer import analyze_stock

# --- [ìœ í‹¸ë¦¬í‹°] ì§€í‘œ ì•„ì´ì½˜ íŒë³„ ---
def get_brief_icon(name, val):
    if val is None: return "âšª"
    if name == "score":
        if val >= 70: return "ğŸ’"
        if val <= 30: return "âš ï¸"
        return "ğŸ“ˆ" if val >= 50 else "ğŸ“‰"
    return ""

# --- [ìœ í‹¸ë¦¬í‹°] ì£¼ê°€ ì •ë³´ ë° í€€íŠ¸ ë¶„ì„ ì¡°íšŒ ---
def get_stock_info(keyword, ticker_map):
    ticker = ticker_map.get(keyword)
    if not ticker: return ""
    try:
        # 1ë…„ì¹˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (í€€íŠ¸ ì—”ì§„ìš©)
        stock = yf.Ticker(ticker)
        df = stock.history(period="1y")
        if df.empty: return ""
        
        # í€€íŠ¸ ë¶„ì„ ìˆ˜í–‰
        m = analyze_stock(df)
        if "error" in m: return f"\nâš ï¸ {keyword}: ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë¶„ì„ ë¶ˆê°€\n"

        price = m['price']
        # ì „ì¼ ì¢…ê°€ ê¸°ë°˜ ë“±ë½ ê³„ì‚° (yf match)
        prev_price = df['Close'].iloc[-2] if len(df) >= 2 else price
        change_pct = ((price - prev_price) / prev_price) * 100
        emoji = "ğŸ”º" if change_pct > 0 else "ğŸ¦‹" if change_pct < 0 else "â–"
        
        # í€€íŠ¸ ìŠ¤ì½”ì–´ ë° ì£¼ìš” ì§€í‘œ ìš”ì•½
        score_icon = get_brief_icon("score", m['score'])
        
        result = f"\nğŸ“Š <b>{keyword} í€€íŠ¸ ë¸Œë¦¬í•‘</b>\n{'-'*20}\n"
        result += f"í˜„ì¬ê°€: {price:,.0f} ({emoji} {change_pct:.2f}%)\n"
        result += f"ì¢…í•©ì ìˆ˜: {score_icon} <b>{m['score']}ì </b>\n"
        
        # ì£¼ìš” ì§€í‘œ 1ì¤„ ìš”ì•½
        rsi_val = f"{m['rsi']:.0f}" if m['rsi'] else "-"
        vol_val = f"{m['volume_ratio']:.0f}%" if m['volume_ratio'] else "-"
        result += f"RSI: {rsi_val} | ìˆ˜ê¸‰: {vol_val} | 52ì£¼: {m['position_52w']:.0f}%\n"
        
        if m['stop_loss']:
            result += f"ì¶”ì²œì†ì ˆ: ğŸ›¡ï¸ {m['stop_loss']:,.0f}ì›\n"
        
        return result + "\n"
    except Exception as e:
        print(f"Stock Info Error ({keyword}): {e}")
        return ""

# --- [í•µì‹¬] ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„ (êµ¬ê¸€ + ë¹™) ---
def fetch_rss_items(keyword):
    """êµ¬ê¸€ ë‰´ìŠ¤ë¥¼ ë¨¼ì € í„¸ê³ , ì—†ìœ¼ë©´ ë¹™ ë‰´ìŠ¤ë¥¼ í…ë‹ˆë‹¤."""
    encoded = urllib.parse.quote(keyword)
    items = []
    
    # 1. ê²€ìƒ‰ ì—”ì§„ ë¦¬ìŠ¤íŠ¸ (ìš°ì„ ìˆœìœ„: êµ¬ê¸€ -> ë¹™)
    search_urls = [
        ("Google", f"https://news.google.com/rss/search?q={encoded}&hl=ko&gl=KR&ceid=KR:ko"),
        ("Bing", f"https://www.bing.com/news/search?q={encoded}&format=rss")
    ]
    
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

    for source_name, url in search_urls:
        if len(items) >= 4: break # ì´ë¯¸ ì¶©ë¶„í•˜ë©´ ì¤‘ë‹¨
        try:
            print(f"ğŸ“¡ {source_name} ê²€ìƒ‰ ì‹œë„...")
            res = requests.get(url, headers=headers, timeout=5)
            soup = BeautifulSoup(res.text, "xml")
            found_items = soup.find_all("item")
            
            for item in found_items:
                title = item.title.get_text()
                link = item.link.get_text()
                # RSSì— ìˆëŠ” ìš”ì•½ë³¸(description) ì¶”ì¶œ
                snippet = ""
                if item.description:
                    snippet = BeautifulSoup(item.description.get_text(), "html.parser").get_text()
                
                items.append({"source": source_name, "title": title, "link": link, "snippet": snippet})
                if len(items) >= 4: break
        except Exception as e:
            print(f"âš ï¸ {source_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
    return items

# --- [í•µì‹¬] ë³¸ë¬¸ ì¶”ì¶œ ì—”ì§„ (Trafilatura + Newspaper3k) ---
def get_article_content(url):
    """ì—¬ëŸ¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë™ì›í•´ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„"""
    # 1. Trafilatura ì‹œë„ (ê°€ì¥ ê¹”ë”í•¨)
    try:
        d = trafilatura.fetch_url(url)
        if d:
            t = trafilatura.extract(d, include_comments=False, include_tables=False)
            if t and len(t) > 50: return t[:1500]
    except: pass
    
    # 2. Newspaper3k ì‹œë„ (ì „í†µì˜ ê°•ì)
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        a = Article(url, language='ko', browser_user_agent=headers['User-Agent'])
        a.download()
        a.parse()
        if len(a.text) > 50: return a.text[:1500]
    except: pass

    return None # ë‹¤ ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜

# --- [í•µì‹¬] AI ìš”ì•½ (í•œê¸€ ê°•ì œ) ---
def get_gemini_summary(keyword, text_data):
    if not GEMINI_API_KEY: return "âš ï¸ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        client = genai.Client(api_key=GEMINI_API_KEY)
        prompt = f"""
        ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ í€ë“œë§¤ë‹ˆì €ì´ì ì‹œì¥ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
        ì œê³µëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{keyword}' ì¢…ëª©ì— ëŒ€í•œ íˆ¬ì ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.

        [ì§€ì‹œ ì‚¬í•­]
        1. ì–¸ì–´: **ë¬´ì¡°ê±´ í•œêµ­ì–´(Korean)**ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
        2. ì–´ì¡°: ì „ë¬¸ì ì´ê³  ê°ê´€ì ì´ë˜, ì •ì¤‘í•œ 'í•´ìš”ì²´'ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
        3. ì„œì‹: ì¤‘ìš” ìˆ«ìë‚˜ í‚¤ì›Œë“œëŠ” <b>íƒœê·¸ë¡œ êµµê²Œ í‘œì‹œí•˜ì‹­ì‹œì˜¤.

        [ì¶œë ¥ ì–‘ì‹]
        Part 1: âš¡ **3ì¤„ í•µì‹¬ ìš”ì•½** (ì´ëª¨ì§€ í™œìš©, í•µì‹¬ ì´ìŠˆ ìœ„ì£¼)
        Part 2: ğŸ“ **ìƒì„¸ ì‹œì¥ íë¦„** (300ì ë‚´ì™¸, ë“±ë½ì˜ ì›ì¸ê³¼ ë°°ê²½ ì„¤ëª…)

        [ë‰´ìŠ¤ ë°ì´í„°]
        {text_data}
        """
        return client.models.generate_content(model="gemini-2.0-flash", contents=prompt).text
    except Exception as e: return f"AI Error: {e}"

# --- ë©”ì¸ ë¡œì§ ---
def process_keyword(keyword, ticker_map):
    print(f"ğŸš€ Analyzing: {keyword}")
    today = datetime.datetime.now().strftime("%y/%m/%d")
    stock_msg = get_stock_info(keyword, ticker_map)
    
    # 1. ë‰´ìŠ¤ ìˆ˜ì§‘ (êµ¬ê¸€ -> ë¹™)
    news_items = fetch_rss_items(keyword)
    
    if not news_items: 
        return f"ğŸ’¤ {keyword}: ë‰´ìŠ¤ ì—†ìŒ (Google & Bing ëª¨ë‘ ì‹¤íŒ¨)"

    llm_input = []
    news_links = []
    
    # 2. ë³¸ë¬¸ ì¶”ì¶œ ë° ë°ì´í„° ì¡°ë¦½
    for i, item in enumerate(news_items):
        title = item['title']
        link = item['link']
        snippet = item['snippet']
        source = item['source']
        
        news_links.append(f"{i+1}. [{source}] <a href='{link}'>{title}</a>")
        
        # ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
        content = get_article_content(link)
        
        if content:
            # ë³¸ë¬¸ ì„±ê³µ ì‹œ
            llm_input.append(f"[ê¸°ì‚¬ {i+1}] ì œëª©: {title}\në‚´ìš©: {content}\n")
        else:
            # ë³¸ë¬¸ ì‹¤íŒ¨ ì‹œ -> RSS Snippet(ìš”ì•½) ì‚¬ìš©
            llm_input.append(f"[ê¸°ì‚¬ {i+1}] ì œëª©: {title}\nìš”ì•½(ì ‘ì†ë¶ˆê°€): {snippet}\n")

    # 3. AI ë¶„ì„
    # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê²½ê³ í•˜ì§€ë§Œ, snippetì´ë¼ë„ ìˆìœ¼ë©´ ì§„í–‰
    full_text = "\n".join(llm_input)
    if len(full_text) < 30:
        return f"âš ï¸ {keyword}: ë¶„ì„í•  ë°ì´í„° ë¶€ì¡±"

    summary = get_gemini_summary(keyword, full_text)
    
    msg = f"ğŸ”¥ <b>[{today}] {keyword} ë¸Œë¦¬í•‘</b> ğŸ”¥\n{stock_msg}{summary}\n\n<b>ğŸ“° ì£¼ìš” ë‰´ìŠ¤</b>\n" + "\n".join(news_links)
    send_telegram(msg)
    return f"âœ… {keyword} ë¸Œë¦¬í•‘ ì™„ë£Œ"

# --- ì•± ì—°ë™ìš© ---
def run_batch_briefing():
    targets, ticker_map = get_db_data()
    logs = []
    if not targets: return ["âš ï¸ í™œì„±í™”ëœ íƒ€ê²Ÿì´ ì—†ìŠµë‹ˆë‹¤."]
    
    for word in targets:
        try:
            log = process_keyword(word, ticker_map)
            logs.append(log)
        except Exception as e:
            logs.append(f"âŒ {word} ì—ëŸ¬: {e}")
        time.sleep(2)
    return logs

if __name__ == "__main__":
    run_batch_briefing()